{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Analysis of Implemented Players for Quixo \n",
    "\n",
    "In this notebook, we will analyze the performance of various algorithms implemented for the game **Quixo**. The players included in the comparison are:\n",
    "\n",
    "- **MinMaxPlayer**: Uses the Minimax algorithm with alpha-beta pruning to make optimal decisions.\n",
    "- **MonteCarloPlayer**: Based on Monte Carlo simulations to evaluate possible moves.\n",
    "- **QLearningPlayer**: An agent that learns using the Q-learning algorithm, a reinforcement learning method.\n",
    "\n",
    "## 1. Initial Setup\n",
    "\n",
    "Below is the initial setup for each player. Each player was tested in a series of matches against different opponents to evaluate their performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from game import Game\n",
    "\n",
    "from players.randomPlayer import RandomPlayer\n",
    "from players.myPlayer import MyPlayer\n",
    "from players.minmaxPlayer import MinMaxPlayer\n",
    "from players.montecarloPlayer import MonteCarloPlayer\n",
    "from players.qlearningPlayer import QLearningPlayer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Match with RandomPlayer\n",
    "\n",
    "To start, let's demonstrate a simple match using the **RandomPlayer**. This player selects moves at random, providing a baseline for understanding the game's mechanics and the performance of more sophisticated strategies.\n",
    "\n",
    "Below, we simulate a single game to observe how the RandomPlayer operates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Game(showPrint=True)    \n",
    "g.print()\n",
    "    \n",
    "\n",
    "myPlayer = MyPlayer()       \n",
    "playerRand = RandomPlayer()       \n",
    "    \n",
    "winner = g.play(myPlayer, playerRand) \n",
    "g.print()\n",
    "\n",
    "print(f\"Winner: Player {winner} {'‚ùå' if winner == 0 else 'üî¥'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Game(showPrint=True)    \n",
    "g.print()\n",
    "    \n",
    "\n",
    "playerMinMax = MinMaxPlayer()       \n",
    "playerRand1 = RandomPlayer()       \n",
    "    \n",
    "winner = g.play(playerMinMax, playerRand1) \n",
    "g.print()\n",
    "\n",
    "print(f\"Winner: Player {winner} {'‚ùå' if winner == 0 else 'üî¥'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Game(showPrint=True)    \n",
    "g.print()\n",
    "    \n",
    "\n",
    "playerMonteCarlo = MonteCarloPlayer()       \n",
    "playerRand2 = RandomPlayer()       \n",
    "    \n",
    "winner = g.play(playerMonteCarlo, playerRand2) \n",
    "g.print()\n",
    "\n",
    "print(f\"Winner: Player {winner} {'‚ùå' if winner == 0 else 'üî¥'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Game(showPrint=True)    \n",
    "g.print()\n",
    "    \n",
    "\n",
    "playerQL = QLearningPlayer()       \n",
    "playerRand3 = RandomPlayer()       \n",
    "    \n",
    "winner = g.play(playerQL, playerRand3) \n",
    "g.print()\n",
    "\n",
    "print(f\"Winner: Player {winner} {'‚ùå' if winner == 0 else 'üî¥'}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Matches\n",
    "I conducted a total of 1000 matches for each player against various opponents to obtain a statistically significant sample. Each player played as both the first and second player to avoid biases due to the starting position."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MinMaxPlayer vs RandomPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MonteCarloPlayer vs RandomPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLearningPlayer vs RandomPlayer\n",
    "\n",
    "In questa sessione analizziamo il player implementato tramite algoritmo qlearning sia come player 0 (cio√® che muove per prima) sia come player1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLearningPlayer-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLearningPlayer-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5 (main, Sep 11 2023, 08:31:25) [Clang 14.0.6 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6a5050be680de1ef3655358b7b0c0068330f61d2a0f8b340ae8220dfb3d86d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
