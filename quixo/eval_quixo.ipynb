{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Analysis of Implemented Players for Quixo \n",
    "\n",
    "In this notebook, we will analyze the performance of various algorithms implemented for the game **Quixo**. The players included in the comparison are:\n",
    "\n",
    "- **MinMaxPlayer**: Uses the Minimax algorithm with alpha-beta pruning to make optimal decisions.\n",
    "- **MonteCarloPlayer**: Based on Monte Carlo simulations to evaluate possible moves.\n",
    "- **QLearningPlayer**: An agent that learns using the Q-learning algorithm, a reinforcement learning method.\n",
    "\n",
    "## 1. Initial Setup\n",
    "\n",
    "Below is the initial setup for each player. Each player was tested in a series of matches against different opponents to evaluate their performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from game import Game\n",
    "\n",
    "from players.randomPlayer import RandomPlayer\n",
    "from players.myPlayer import MyPlayer\n",
    "from players.minmaxPlayer import MinMaxPlayer\n",
    "from players.montecarloPlayer import MonteCarloPlayer\n",
    "from players.qlearningPlayer import QLearningPlayer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Match with RandomPlayer\n",
    "\n",
    "To start, let's demonstrate a simple match using the **RandomPlayer**. This player selects moves at random, providing a baseline for understanding the game's mechanics and the performance of more sophisticated strategies.\n",
    "\n",
    "Below, we simulate a single game to observe how the RandomPlayer operates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****************\n",
      "\n",
      "⬜ ⬜ ⬜ ⬜ ⬜ \n",
      "⬜ ⬜ ⬜ ⬜ ⬜ \n",
      "⬜ ⬜ ⬜ ⬜ ⬜ \n",
      "⬜ ⬜ ⬜ ⬜ ⬜ \n",
      "⬜ ⬜ ⬜ ⬜ ⬜ \n",
      "\n",
      "\n",
      "*****************\n",
      "\n",
      "🔴 ❌ ❌ ❌ ❌ \n",
      "🔴 ❌ 🔴 ⬜ ❌ \n",
      "🔴 🔴 🔴 🔴 🔴 \n",
      "❌ ❌ ⬜ 🔴 ❌ \n",
      "❌ ⬜ 🔴 🔴 ❌ \n",
      "\n",
      "Winner: Player 1 🔴\n"
     ]
    }
   ],
   "source": [
    "g = Game(showPrint=True)    \n",
    "g.print()\n",
    "    \n",
    "\n",
    "myPlayer = MyPlayer()       \n",
    "playerRand = RandomPlayer()       \n",
    "    \n",
    "winner = g.play(myPlayer, playerRand) \n",
    "g.print()\n",
    "\n",
    "print(f\"Winner: Player {winner} {'❌' if winner == 0 else '🔴'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****************\n",
      "\n",
      "⬜ ⬜ ⬜ ⬜ ⬜ \n",
      "⬜ ⬜ ⬜ ⬜ ⬜ \n",
      "⬜ ⬜ ⬜ ⬜ ⬜ \n",
      "⬜ ⬜ ⬜ ⬜ ⬜ \n",
      "⬜ ⬜ ⬜ ⬜ ⬜ \n",
      "\n",
      "\n",
      "*****************\n",
      "\n",
      "🔴 🔴 🔴 🔴 ❌ \n",
      "⬜ ⬜ ⬜ ⬜ ❌ \n",
      "⬜ ⬜ ⬜ ⬜ ❌ \n",
      "🔴 ⬜ ⬜ ⬜ ❌ \n",
      "🔴 ❌ ⬜ 🔴 ❌ \n",
      "\n",
      "Winner: Player 0 ❌\n"
     ]
    }
   ],
   "source": [
    "g = Game(showPrint=True)    \n",
    "g.print()\n",
    "    \n",
    "\n",
    "playerMinMax = MinMaxPlayer()       \n",
    "playerRand1 = RandomPlayer()       \n",
    "    \n",
    "winner = g.play(playerMinMax, playerRand1) \n",
    "g.print()\n",
    "\n",
    "print(f\"Winner: Player {winner} {'❌' if winner == 0 else '🔴'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****************\n",
      "\n",
      "⬜ ⬜ ⬜ ⬜ ⬜ \n",
      "⬜ ⬜ ⬜ ⬜ ⬜ \n",
      "⬜ ⬜ ⬜ ⬜ ⬜ \n",
      "⬜ ⬜ ⬜ ⬜ ⬜ \n",
      "⬜ ⬜ ⬜ ⬜ ⬜ \n",
      "\n",
      "\n",
      "*****************\n",
      "\n",
      "🔴 🔴 ⬜ ⬜ ❌ \n",
      "⬜ ⬜ ⬜ ⬜ ❌ \n",
      "⬜ ⬜ ⬜ ⬜ ❌ \n",
      "🔴 ⬜ ⬜ ⬜ ❌ \n",
      "🔴 ⬜ ⬜ ⬜ ❌ \n",
      "\n",
      "Winner: Player 0 ❌\n"
     ]
    }
   ],
   "source": [
    "g = Game(showPrint=True)    \n",
    "g.print()\n",
    "    \n",
    "\n",
    "playerMonteCarlo = MonteCarloPlayer()       \n",
    "playerRand2 = RandomPlayer()       \n",
    "    \n",
    "winner = g.play(playerMonteCarlo, playerRand2) \n",
    "g.print()\n",
    "\n",
    "print(f\"Winner: Player {winner} {'❌' if winner == 0 else '🔴'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Game(showPrint=True)    \n",
    "g.print()\n",
    "    \n",
    "\n",
    "playerQL = QLearningPlayer()       \n",
    "playerRand3 = RandomPlayer()       \n",
    "    \n",
    "winner = g.play(playerQL, playerRand3) \n",
    "g.print()\n",
    "\n",
    "print(f\"Winner: Player {winner} {'❌' if winner == 0 else '🔴'}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Matches\n",
    "I conducted a total of 1000 matches for each player against various opponents to obtain a statistically significant sample. Each player played as both the first and second player to avoid biases due to the starting position."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MinMaxPlayer vs RandomPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:48<00:00,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result:\n",
      "\tWin Rate MinMaxPlayer: 59.50%\n",
      "\tWin Rate RandomPlayer: 40.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "win0 =0\n",
    "win1 =0\n",
    "nGame = 1000\n",
    "\n",
    "for game in tqdm(range(nGame)):\n",
    "    g = Game(showPrint = False)\n",
    "\n",
    "    player0 = MinMaxPlayer()\n",
    "    player1 = RandomPlayer()\n",
    "    winner = g.play(player0, player1)\n",
    "    \n",
    "    if winner:\n",
    "        win1+=1\n",
    "    else: \n",
    "        win0+=1\n",
    "    \n",
    "print(\"\\nResult:\")\n",
    "print(\"\\tWin Rate MinMaxPlayer: {:.2f}%\".format((win0 / nGame) * 100))  \n",
    "print(\"\\tWin Rate RandomPlayer: {:.2f}%\".format((win1 / nGame) * 100)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MonteCarloPlayer vs RandomPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [03:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m player0 \u001b[39m=\u001b[39m MonteCarloPlayer()\n\u001b[1;32m      9\u001b[0m player1 \u001b[39m=\u001b[39m RandomPlayer()\n\u001b[0;32m---> 10\u001b[0m winner \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39mplay(player0, player1)\n\u001b[1;32m     12\u001b[0m \u001b[39mif\u001b[39;00m winner:\n\u001b[1;32m     13\u001b[0m     win1\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Computational_Intelligence24/quixo/game.py:117\u001b[0m, in \u001b[0;36mGame.play\u001b[0;34m(self, player1, player2)\u001b[0m\n\u001b[1;32m    115\u001b[0m ok \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m ok:\n\u001b[0;32m--> 117\u001b[0m     from_pos, slide \u001b[39m=\u001b[39m players[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_player_idx]\u001b[39m.\u001b[39mmake_move(\n\u001b[1;32m    118\u001b[0m         \u001b[39mself\u001b[39m)\n\u001b[1;32m    119\u001b[0m     ok \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__move(from_pos, slide, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_player_idx)\n\u001b[1;32m    120\u001b[0m winner \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_winner()\n",
      "File \u001b[0;32m~/Desktop/Computational_Intelligence24/quixo/players/montecarloPlayer.py:13\u001b[0m, in \u001b[0;36mMonteCarloPlayer.make_move\u001b[0;34m(self, game)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_move\u001b[39m(\u001b[39mself\u001b[39m, game: \u001b[39m'\u001b[39m\u001b[39mGame\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[\u001b[39mtuple\u001b[39m[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m], Move]:\n\u001b[0;32m---> 13\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmonte_carlo_move(game)\n",
      "File \u001b[0;32m~/Desktop/Computational_Intelligence24/quixo/players/montecarloPlayer.py:23\u001b[0m, in \u001b[0;36mMonteCarloPlayer.monte_carlo_move\u001b[0;34m(self, game)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39m# Iterare su un sottoinsieme casuale di mosse disponibili\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m move \u001b[39min\u001b[39;00m random\u001b[39m.\u001b[39msample(available_moves, num_selected_moves):\n\u001b[0;32m---> 23\u001b[0m     total_score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msimulate_move(game, move)\n\u001b[1;32m     25\u001b[0m     \u001b[39m# Aggiorna la migliore mossa in base al punteggio totale\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[39mif\u001b[39;00m total_score \u001b[39m>\u001b[39m best_score:\n",
      "File \u001b[0;32m~/Desktop/Computational_Intelligence24/quixo/players/montecarloPlayer.py:43\u001b[0m, in \u001b[0;36mMonteCarloPlayer.simulate_move\u001b[0;34m(self, game, move)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39m# Simula il gioco fino a quando non c'è un vincitore\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mwhile\u001b[39;00m winner \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[0;32m---> 43\u001b[0m     random_move \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mchoice(cloned_game\u001b[39m.\u001b[39mavailable_moves(cloned_game\u001b[39m.\u001b[39mget_current_player()))\n\u001b[1;32m     44\u001b[0m     cloned_game\u001b[39m.\u001b[39mexecute_move(random_move[\u001b[39m0\u001b[39m], random_move[\u001b[39m1\u001b[39m], cloned_game\u001b[39m.\u001b[39mget_current_player())\n\u001b[1;32m     45\u001b[0m     winner \u001b[39m=\u001b[39m cloned_game\u001b[39m.\u001b[39mcheck_winner()\n",
      "File \u001b[0;32m~/Desktop/Computational_Intelligence24/quixo/game.py:251\u001b[0m, in \u001b[0;36mGame.available_moves\u001b[0;34m(self, player_idx)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mfor\u001b[39;00m from_pos \u001b[39min\u001b[39;00m edge_positions:\n\u001b[1;32m    250\u001b[0m     \u001b[39mfor\u001b[39;00m slide \u001b[39min\u001b[39;00m Move:\n\u001b[0;32m--> 251\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_move(from_pos, slide, player_idx):\n\u001b[1;32m    252\u001b[0m             possible_moves\u001b[39m.\u001b[39mappend((from_pos, slide))\n\u001b[1;32m    254\u001b[0m \u001b[39mreturn\u001b[39;00m possible_moves\n",
      "File \u001b[0;32m~/Desktop/Computational_Intelligence24/quixo/game.py:279\u001b[0m, in \u001b[0;36mGame.check_move\u001b[0;34m(self, from_pos, slide, player_id)\u001b[0m\n\u001b[1;32m    272\u001b[0m     acceptable_slide \u001b[39m=\u001b[39m (app \u001b[39m==\u001b[39m (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39mand\u001b[39;00m slide \u001b[39min\u001b[39;00m {Move\u001b[39m.\u001b[39mBOTTOM, Move\u001b[39m.\u001b[39mRIGHT}) \u001b[39mor\u001b[39;00m \\\n\u001b[1;32m    273\u001b[0m                     (app \u001b[39m==\u001b[39m (\u001b[39m4\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39mand\u001b[39;00m slide \u001b[39min\u001b[39;00m {Move\u001b[39m.\u001b[39mTOP, Move\u001b[39m.\u001b[39mRIGHT}) \u001b[39mor\u001b[39;00m \\\n\u001b[1;32m    274\u001b[0m                     (app \u001b[39m==\u001b[39m (\u001b[39m0\u001b[39m, \u001b[39m4\u001b[39m) \u001b[39mand\u001b[39;00m slide \u001b[39min\u001b[39;00m {Move\u001b[39m.\u001b[39mBOTTOM, Move\u001b[39m.\u001b[39mLEFT}) \u001b[39mor\u001b[39;00m \\\n\u001b[1;32m    275\u001b[0m                     (app \u001b[39m==\u001b[39m (\u001b[39m4\u001b[39m, \u001b[39m4\u001b[39m) \u001b[39mand\u001b[39;00m slide \u001b[39min\u001b[39;00m {Move\u001b[39m.\u001b[39mTOP, Move\u001b[39m.\u001b[39mLEFT})\n\u001b[1;32m    276\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m     acceptable_slide \u001b[39m=\u001b[39m (app[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m slide \u001b[39min\u001b[39;00m {Move\u001b[39m.\u001b[39mBOTTOM, Move\u001b[39m.\u001b[39mLEFT, Move\u001b[39m.\u001b[39mRIGHT}) \u001b[39mor\u001b[39;00m \\\n\u001b[1;32m    278\u001b[0m                     (app[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m \u001b[39mand\u001b[39;00m slide \u001b[39min\u001b[39;00m {Move\u001b[39m.\u001b[39mTOP, Move\u001b[39m.\u001b[39mLEFT, Move\u001b[39m.\u001b[39mRIGHT}) \u001b[39mor\u001b[39;00m \\\n\u001b[0;32m--> 279\u001b[0m                     (app[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m slide \u001b[39min\u001b[39;00m {Move\u001b[39m.\u001b[39mBOTTOM, Move\u001b[39m.\u001b[39mTOP, Move\u001b[39m.\u001b[39mRIGHT}) \u001b[39mor\u001b[39;00m \\\n\u001b[1;32m    280\u001b[0m                     (app[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m \u001b[39mand\u001b[39;00m slide \u001b[39min\u001b[39;00m {Move\u001b[39m.\u001b[39mBOTTOM, Move\u001b[39m.\u001b[39mTOP, Move\u001b[39m.\u001b[39mLEFT})\n\u001b[1;32m    282\u001b[0m \u001b[39mreturn\u001b[39;00m acceptable_take \u001b[39mand\u001b[39;00m acceptable_slide\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/enum.py:1224\u001b[0m, in \u001b[0;36mEnum.__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__format__\u001b[39m(\u001b[39mself\u001b[39m, format_spec):\n\u001b[1;32m   1222\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mstr\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__format__\u001b[39m(\u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m), format_spec)\n\u001b[0;32m-> 1224\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__hash__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1225\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mhash\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name_)\n\u001b[1;32m   1227\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__reduce_ex__\u001b[39m(\u001b[39mself\u001b[39m, proto):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "win0 =0\n",
    "win1 =0\n",
    "nGame = 1000\n",
    "\n",
    "for game in tqdm(range(nGame)):\n",
    "    g = Game(showPrint = False)\n",
    "\n",
    "    player0 = MonteCarloPlayer()\n",
    "    player1 = RandomPlayer()\n",
    "    winner = g.play(player0, player1)\n",
    "    \n",
    "    if winner:\n",
    "        win1+=1\n",
    "    else: \n",
    "        win0+=1\n",
    "    \n",
    "print(\"\\nResult:\")\n",
    "print(\"\\tWin Rate MonteCarloPlayer: {:.2f}%\".format((win0 / nGame) * 100))  \n",
    "print(\"\\tWin Rate RandomPlayer: {:.2f}%\".format((win1 / nGame) * 100)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLearningPlayer vs RandomPlayer\n",
    "\n",
    "In questa sessione analizziamo il player implementato tramite algoritmo qlearning sia come player 0 (cioè che muove per prima) sia come player1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLearningPlayer-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win0 =0\n",
    "win1 =0\n",
    "nGame = 1000\n",
    "\n",
    "for game in tqdm(range(nGame)):\n",
    "    g = Game(showPrint = False)\n",
    "\n",
    "    player0 = QLearningPlayer(0)\n",
    "    player1 = RandomPlayer()\n",
    "    winner = g.play(player0, player1)\n",
    "    \n",
    "    if winner:\n",
    "        win1+=1\n",
    "    else: \n",
    "        win0+=1\n",
    "    \n",
    "print(\"\\nResult:\")\n",
    "print(\"\\tWin Rate QLearningPlayer: {:.2f}%\".format((win0 / nGame) * 100))  \n",
    "print(\"\\tWin Rate RandomPlayer: {:.2f}%\".format((win1 / nGame) * 100)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLearningPlayer-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win0 =0\n",
    "win1 =0\n",
    "nGame = 1000\n",
    "\n",
    "for game in tqdm(range(nGame)):\n",
    "    g = Game(showPrint = False)\n",
    "\n",
    "    player0 = RandomPlayer()\n",
    "    player1 = QLearningPlayer(1)\n",
    "    winner = g.play(player0, player1)\n",
    "    \n",
    "    if winner:\n",
    "        win1+=1\n",
    "    else: \n",
    "        win0+=1\n",
    "    \n",
    "print(\"\\nResult:\")\n",
    "print(\"\\tWin Rate RandomPlayer: {:.2f}%\".format((win0 / nGame) * 100))  \n",
    "print(\"\\tWin Rate QLearningPlayer: {:.2f}%\".format((win1 / nGame) * 100)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tournament Among Players\n",
    "\n",
    "In this section, we will conduct a tournament to evaluate the performance of different players implemented for the game of Quixo. The players participating in this tournament include:\n",
    "\n",
    "- **RandomPlayer**: A player that makes decisions randomly, serving as a baseline for comparison.\n",
    "- **MyPlayer**: A custom player that uses a personalized strategy to play the game.\n",
    "- **MinMaxPlayer**: A player that uses the Minimax algorithm with alpha-beta pruning to make optimal decisions.\n",
    "- **MonteCarloPlayer**: A player that employs Monte Carlo Tree Search to explore possible moves and outcomes.\n",
    "- **QLearningPlayer**: A player that utilizes Q-learning, a reinforcement learning algorithm, to improve its strategy over time.\n",
    "\n",
    "We will simulate a series of matches between these players to determine which strategy performs best. The results will provide insights into their strengths and weaknesses, allowing us to better understand their gameplay dynamics.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "|                                         |                                    | Win                             |\n",
    "|-----------------------------------------|------------------------------------|---------------------------------|\n",
    "| 1. RandomPlayer vs MinMaxPlayer         |                                    |                                 |\n",
    "|                                         | win1 vs win2                       |                                 |\n",
    "| 2. MonteCarloPlayer vs QLearningPlayer  |                                    | champion                        |\n",
    "|                                         | win2 vs win3                       |                                 |\n",
    "| 3. MyPlayer vs MonteCarloPlayer         |                                    |                                 |\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6a5050be680de1ef3655358b7b0c0068330f61d2a0f8b340ae8220dfb3d86d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
