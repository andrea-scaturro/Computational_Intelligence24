{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Analysis of Implemented Players for Quixo \n",
    "\n",
    "In this notebook, we will analyze the performance of various algorithms implemented for the game **Quixo**. The players included in the comparison are:\n",
    "\n",
    "- **MinMaxPlayer**: Uses the Minimax algorithm with alpha-beta pruning to make optimal decisions.\n",
    "- **MonteCarloPlayer**: Based on Monte Carlo simulations to evaluate possible moves.\n",
    "- **QLearningPlayer**: An agent that learns using the Q-learning algorithm, a reinforcement learning method.\n",
    "\n",
    "## 1. Initial Setup\n",
    "\n",
    "Below is the initial setup for each player. Each player was tested in a series of matches against different opponents to evaluate their performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from game import Game\n",
    "\n",
    "from players.randomPlayer import RandomPlayer\n",
    "from players.myPlayer import MyPlayer\n",
    "from players.minmaxPlayer import MinMaxPlayer\n",
    "from players.montecarloPlayer import MonteCarloPlayer\n",
    "from players.qlearningPlayer import QLearningPlayer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Match with RandomPlayer\n",
    "\n",
    "To start, let's demonstrate a simple match using the **RandomPlayer**. This player selects moves at random, providing a baseline for understanding the game's mechanics and the performance of more sophisticated strategies.\n",
    "\n",
    "Below, we simulate a single game to observe how the RandomPlayer operates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****************\n",
      "\n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "\n",
      "\n",
      "*****************\n",
      "\n",
      "ğŸ”´ âŒ âŒ âŒ âŒ \n",
      "ğŸ”´ âŒ ğŸ”´ â¬œ âŒ \n",
      "ğŸ”´ ğŸ”´ ğŸ”´ ğŸ”´ ğŸ”´ \n",
      "âŒ âŒ â¬œ ğŸ”´ âŒ \n",
      "âŒ â¬œ ğŸ”´ ğŸ”´ âŒ \n",
      "\n",
      "Winner: Player 1 ğŸ”´\n"
     ]
    }
   ],
   "source": [
    "g = Game(showPrint=True)    \n",
    "g.print()\n",
    "    \n",
    "\n",
    "myPlayer = MyPlayer()       \n",
    "playerRand = RandomPlayer()       \n",
    "    \n",
    "winner = g.play(myPlayer, playerRand) \n",
    "g.print()\n",
    "\n",
    "print(f\"Winner: Player {winner} {'âŒ' if winner == 0 else 'ğŸ”´'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****************\n",
      "\n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "\n",
      "\n",
      "*****************\n",
      "\n",
      "ğŸ”´ ğŸ”´ ğŸ”´ ğŸ”´ âŒ \n",
      "â¬œ â¬œ â¬œ â¬œ âŒ \n",
      "â¬œ â¬œ â¬œ â¬œ âŒ \n",
      "ğŸ”´ â¬œ â¬œ â¬œ âŒ \n",
      "ğŸ”´ âŒ â¬œ ğŸ”´ âŒ \n",
      "\n",
      "Winner: Player 0 âŒ\n"
     ]
    }
   ],
   "source": [
    "g = Game(showPrint=True)    \n",
    "g.print()\n",
    "    \n",
    "\n",
    "playerMinMax = MinMaxPlayer()       \n",
    "playerRand1 = RandomPlayer()       \n",
    "    \n",
    "winner = g.play(playerMinMax, playerRand1) \n",
    "g.print()\n",
    "\n",
    "print(f\"Winner: Player {winner} {'âŒ' if winner == 0 else 'ğŸ”´'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*****************\n",
      "\n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "â¬œ â¬œ â¬œ â¬œ â¬œ \n",
      "\n",
      "\n",
      "*****************\n",
      "\n",
      "ğŸ”´ ğŸ”´ â¬œ â¬œ âŒ \n",
      "â¬œ â¬œ â¬œ â¬œ âŒ \n",
      "â¬œ â¬œ â¬œ â¬œ âŒ \n",
      "ğŸ”´ â¬œ â¬œ â¬œ âŒ \n",
      "ğŸ”´ â¬œ â¬œ â¬œ âŒ \n",
      "\n",
      "Winner: Player 0 âŒ\n"
     ]
    }
   ],
   "source": [
    "g = Game(showPrint=True)    \n",
    "g.print()\n",
    "    \n",
    "\n",
    "playerMonteCarlo = MonteCarloPlayer()       \n",
    "playerRand2 = RandomPlayer()       \n",
    "    \n",
    "winner = g.play(playerMonteCarlo, playerRand2) \n",
    "g.print()\n",
    "\n",
    "print(f\"Winner: Player {winner} {'âŒ' if winner == 0 else 'ğŸ”´'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Game(showPrint=True)    \n",
    "g.print()\n",
    "    \n",
    "\n",
    "playerQL = QLearningPlayer()       \n",
    "playerRand3 = RandomPlayer()       \n",
    "    \n",
    "winner = g.play(playerQL, playerRand3) \n",
    "g.print()\n",
    "\n",
    "print(f\"Winner: Player {winner} {'âŒ' if winner == 0 else 'ğŸ”´'}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Matches\n",
    "I conducted a total of 1000 matches for each player against various opponents to obtain a statistically significant sample. Each player played as both the first and second player to avoid biases due to the starting position."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MinMaxPlayer vs RandomPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [02:48<00:00,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result:\n",
      "\tWin Rate MinMaxPlayer: 59.50%\n",
      "\tWin Rate RandomPlayer: 40.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "win0 =0\n",
    "win1 =0\n",
    "nGame = 1000\n",
    "\n",
    "for game in tqdm(range(nGame)):\n",
    "    g = Game(showPrint = False)\n",
    "\n",
    "    player0 = MinMaxPlayer()\n",
    "    player1 = RandomPlayer()\n",
    "    winner = g.play(player0, player1)\n",
    "    \n",
    "    if winner:\n",
    "        win1+=1\n",
    "    else: \n",
    "        win0+=1\n",
    "    \n",
    "print(\"\\nResult:\")\n",
    "print(\"\\tWin Rate MinMaxPlayer: {:.2f}%\".format((win0 / nGame) * 100))  \n",
    "print(\"\\tWin Rate RandomPlayer: {:.2f}%\".format((win1 / nGame) * 100)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MonteCarloPlayer vs RandomPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win0 =0\n",
    "win1 =0\n",
    "nGame = 1000\n",
    "\n",
    "for game in tqdm(range(nGame)):\n",
    "    g = Game(showPrint = False)\n",
    "\n",
    "    player0 = MonteCarloPlayer()\n",
    "    player1 = RandomPlayer()\n",
    "    winner = g.play(player0, player1)\n",
    "    \n",
    "    if winner:\n",
    "        win1+=1\n",
    "    else: \n",
    "        win0+=1\n",
    "    \n",
    "print(\"\\nResult:\")\n",
    "print(\"\\tWin Rate MonteCarloPlayer: {:.2f}%\".format((win0 / nGame) * 100))  \n",
    "print(\"\\tWin Rate RandomPlayer: {:.2f}%\".format((win1 / nGame) * 100)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLearningPlayer vs RandomPlayer\n",
    "\n",
    "In questa sessione analizziamo il player implementato tramite algoritmo qlearning sia come player 0 (cioÃ¨ che muove per prima) sia come player1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLearningPlayer-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win0 =0\n",
    "win1 =0\n",
    "nGame = 1000\n",
    "\n",
    "for game in tqdm(range(nGame)):\n",
    "    g = Game(showPrint = False)\n",
    "\n",
    "    player0 = QLearningPlayer(0)\n",
    "    player1 = RandomPlayer()\n",
    "    winner = g.play(player0, player1)\n",
    "    \n",
    "    if winner:\n",
    "        win1+=1\n",
    "    else: \n",
    "        win0+=1\n",
    "    \n",
    "print(\"\\nResult:\")\n",
    "print(\"\\tWin Rate QLearningPlayer: {:.2f}%\".format((win0 / nGame) * 100))  \n",
    "print(\"\\tWin Rate RandomPlayer: {:.2f}%\".format((win1 / nGame) * 100)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLearningPlayer-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win0 =0\n",
    "win1 =0\n",
    "nGame = 1000\n",
    "\n",
    "for game in tqdm(range(nGame)):\n",
    "    g = Game(showPrint = False)\n",
    "\n",
    "    player0 = RandomPlayer()\n",
    "    player1 = QLearningPlayer(1)\n",
    "    winner = g.play(player0, player1)\n",
    "    \n",
    "    if winner:\n",
    "        win1+=1\n",
    "    else: \n",
    "        win0+=1\n",
    "    \n",
    "print(\"\\nResult:\")\n",
    "print(\"\\tWin Rate RandomPlayer: {:.2f}%\".format((win0 / nGame) * 100))  \n",
    "print(\"\\tWin Rate QLearningPlayer: {:.2f}%\".format((win1 / nGame) * 100)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tournament Among Players\n",
    "\n",
    "In this section, we will conduct a tournament to evaluate the performance of different players implemented for the game of Quixo. The players participating in this tournament include:\n",
    "\n",
    "- **RandomPlayer**: A player that makes decisions randomly, serving as a baseline for comparison.\n",
    "- **MyPlayer**: A custom player that uses a personalized strategy to play the game.\n",
    "- **MinMaxPlayer**: A player that uses the Minimax algorithm with alpha-beta pruning to make optimal decisions.\n",
    "- **MonteCarloPlayer**: A player that employs Monte Carlo Tree Search to explore possible moves and outcomes.\n",
    "- **QLearningPlayer**: A player that utilizes Q-learning, a reinforcement learning algorithm, to improve its strategy over time.\n",
    "\n",
    "We will simulate a series of matches between these players to determine which strategy performs best. The results will provide insights into their strengths and weaknesses, allowing us to better understand their gameplay dynamics.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "|                                         |                                    | Win                             |\n",
    "|-----------------------------------------|------------------------------------|---------------------------------|\n",
    "| 1. RandomPlayer vs MinMaxPlayer         |                                    |                                 |\n",
    "|                                         | win1 vs win2                       |                                 |\n",
    "| 2. MonteCarloPlayer vs QLearningPlayer  |                                    | champion                        |\n",
    "|                                         | win2 vs win3                       |                                 |\n",
    "| 3. MyPlayer vs MonteCarloPlayer         |                                    |                                 |\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomPlayer vs MinMaxPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win0 =0\n",
    "win1 =0\n",
    "nGame = 1000\n",
    "\n",
    "for game in tqdm(range(nGame)):\n",
    "    g = Game(showPrint = False)\n",
    "\n",
    "    player0 = MinMaxPlayer()\n",
    "    player1 = RandomPlayer()\n",
    "    winner = g.play(player0, player1)\n",
    "    \n",
    "    if winner:\n",
    "        win1+=1\n",
    "    else: \n",
    "        win0+=1\n",
    "    \n",
    "print(\"\\nResult:\")\n",
    "print(\"\\tWin Rate MinMaxPlayer: {:.2f}%\".format((win0 / nGame) * 100))  \n",
    "print(\"\\tWin Rate RandomPlayer: {:.2f}%\".format((win1 / nGame) * 100)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MonteCarloPlayer vs QLearningPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win0 =0\n",
    "win1 =0\n",
    "nGame = 1000\n",
    "\n",
    "for game in tqdm(range(nGame)):\n",
    "    g = Game(showPrint = False)\n",
    "\n",
    "    player0 = MonteCarloPlayer()\n",
    "    player1 = QLearningPlayer(1)\n",
    "    winner = g.play(player0, player1)\n",
    "    \n",
    "    if winner:\n",
    "        win1+=1\n",
    "    else: \n",
    "        win0+=1\n",
    "    \n",
    "print(\"\\nResult:\")\n",
    "print(\"\\tWin Rate MonteCarloPlayer: {:.2f}%\".format((win0 / nGame) * 100))  \n",
    "print(\"\\tWin Rate QLearningPlayer: {:.2f}%\".format((win1 / nGame) * 100)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MyPlayer vs MonteCarloPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win0 =0\n",
    "win1 =0\n",
    "nGame = 1000\n",
    "\n",
    "for game in tqdm(range(nGame)):\n",
    "    g = Game(showPrint = False)\n",
    "\n",
    "    player0 = MonteCarloPlayer()\n",
    "    player1 = MyPlayer()\n",
    "    winner = g.play(player0, player1)\n",
    "    \n",
    "    if winner:\n",
    "        win1+=1\n",
    "    else: \n",
    "        win0+=1\n",
    "    \n",
    "print(\"\\nResult:\")\n",
    "print(\"\\tWin Rate MonteCarloPlayer: {:.2f}%\".format((win0 / nGame) * 100))  \n",
    "print(\"\\tWin Rate MyPlayer: {:.2f}%\".format((win1 / nGame) * 100)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f6a5050be680de1ef3655358b7b0c0068330f61d2a0f8b340ae8220dfb3d86d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
